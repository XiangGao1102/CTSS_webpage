<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>CTSS-ICML2022</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: #0066CC;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
		
		.left {
			text-align: left;
			border: 1px dotted black;
			width: 50%;
		}
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p style="font-size:35px;">Learning to Incorporate Texture Saliency Adaptive Attention to Image Cartoonization</p>

	<a href="mailto:gaoxiang181@mails.ucas.ac.cn" class="links">Xiang Gao </a> &nbsp; &nbsp; 
	<a href="mailto:zhangyuqi201@mails.ucas.ac.cn" class="links">Yuqi Zhang </a> &nbsp; &nbsp; 
	<a href="mailto:tyj@ucas.ac.cn" class="links">Yingjie Tian </a>

	<br>
	<p class="para-3"><span class="p1"> School of Computer Science and Technology, University of Chinese Academy of Sciences</span></p>
	<p class="para-3"><span class="p1"> Accepted by <i>ICML 2022.</i></span></p>
	

	</div>

        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
		<p class='p2'> Abstract </p> 
		<p class='p1'>Image cartoonization is recently dominated by generative adversarial networks (GANs) from the perspective of
unsupervised image-to-image translation, in which an inherent challenge is to precisely capture and sufficiently transfer characteristic cartoon styles (e.g., clear edges, smooth color shading, abstract fine structures, etc.). Existing advanced models try to enhance cartoonization effect by learning to promote edges adversarially, introducing style transfer loss, or learning to align style from multiple representation space. This paper demonstrates that more distinct and vivid cartoonization effect could be easily achieved with only basic adversarial loss. Observing that cartoon style is more evident in cartoon-texture-salient local image regions, we build a region-level adversarial learning branch in parallel with the normal image-level one, which constrains adversarial learning on cartoon-texture-salient local patches for better perceiving and transferring cartoon texture features. To this end, a novel cartoon-texture-saliency-sampler (CTSS) module is proposed to dynamically sample cartoon-texture-salient patches from training data. With extensive experiments, we demonstrate that texture saliency adaptive attention in adversarial learning, as a missing ingredient of related methods in image cartoonization, is of significant importance in facilitating and enhancing image cartoon stylization, especially for high-resolution input pictures.ness and superiority of our method for text-guided I2I are demonstrated with extensive experiments both qualitatively and quantitatively.
            </div>
			
        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
            <p class='p2'> Method </p> 
			<p style="line-height:180%">
			<strong>Key Idea</strong>: 
			<br>
			(1) <b>Global style local texture dual adversarial learning</b>. Recent techniques of image cartoonization are dominated by generative adversarial networks (GANs) from the perspective of image-to-image translation. However, it is difficult for existing methods to produce salient cartoon styles for high-resolution input natural images. To solve this problem, we propose a dual adversarial learning framework which applies an image-level global adversarial learning to transfer global cartoon styles, and meanwhile construct a patch-level local adversarial learning to capture local salient cartoon textures. Such dual adversarial learning architecture contributes to noticeably more salient cartoon style rendering for large input images.
			<br>
			(2) <b>Cartoon-texture-saliency-sampler (CTSS) for adaptive patch sampling</b>. We propose CTSS module to adaptively sample image patches with the most salient cartoon textures from each mini-batch of input training images. The sampled image patches are used as training data of the patch-level local adversarial learning branch. With the texture-saliency-based adaptive attention provided by CTSS, the model's ability of capturing and transferring salient cartoon textures is dramatically enhanced.
			<br>
			(3) <b>Training-free texture saliency dynamic attention</b>. The CTSS is composed of a series of training-free modules, including a convolution-based edge detector, an edge amplifier based on high-pass filtering, and an edge-guided patch sorting operation. These modules together realize dynamic localization and sampling of cartoon-texture-salient local patches, which provides training data of the patch-level adversarial learning branch.
			<br>
			</p>
			
			<br>
            <div style="padding-left: 4%; padding-right: 4%;">
                <div align="center">
                    <img src="img/arch.jpg" width="100%"> <br>
                </div>
            <p style="line-height:180%">Figure 1. The overall architecture of our model, as well as details of our proposed cartoon-texture-saliency-sampler (CTSS) module which adaptively extracts local image patches with most salient cartoon texture pattern from each mini-batch of input images.
			</div>
        
            <p class='p2'> Results </p> 
            <div style="padding-left: 4%; padding-right: 4%;">
                <div align="center">
                    <img src="img/qualitative_results.jpg" width="100%"> <br>
                    <p style="line-height:150%">
					Figure 2. Example image cartoonization results of high-resolution real-world-scene input images. Results are evaluated on our model trained over different cartoon datasets, including “The Wind Rises” (the second row), “Dragon Ball” (the third row), and “Crayon Shin-chan” (the bottom row)
					</p>
                </div>
			<br>

                <div align="center">
                    <img src="img/method_compare_1.jpg" width="100%"> <br>
                    <p style="line-height:150%">
					Figure 3. Comparison of our method with general image stylization or image abstraction methods tested over high-resolution real-world-scene input images. Results are evaluated on “The Wind Rises” dataset.
					</p>
                </div>

                <div align="center">
                    <img src="img/method_compare_2.jpg" width="100%"> <br>
                    <p style="line-height:150%">
					Figure 4. Comparison of our approach with related advanced image cartoonization methods tested over high-resolution real-world-scene input images. Results are evaluated on “The Wind Rises” dataset.
					</p>
                </div>

			</div>

	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:15px">
		　　<li> Paper: <a href="https://arxiv.org/abs/2208.01587" class="links">arXiv version</a> </li>
		   <li> Paper: <a href="https://proceedings.mlr.press/v162/gao22k/gao22k.pdf" class="links">ICML version</a></li>
		　　<li> Code: <a href="https://github.com/xianggao1102/learning-to-incorporate-texture-saliency-adaptive-attention-to-image-cartoonization" class="links">Github</a> </li>
		</ul>
	</p>


	<p class='p2'> Citation</p>
	<p> 
		@article{gao2022learning, <br>
			&nbsp; &nbsp; title={Learning to Incorporate Texture Saliency Adaptive Attention to Image Cartoonization},<br>
			&nbsp; &nbsp; author={Gao, Xiang and Zhang, Yuqi and Tian, Yingjie},<br>
			&nbsp; &nbsp; booktitle={International Conference on Machine Learning},<br>
			&nbsp; &nbsp; year={2022}, <br>
			&nbsp; &nbsp; pages={7183--7207} <br>
			}
	</p>
		

</html>
